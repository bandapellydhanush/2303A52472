{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQ0BZb5ZzOFqPAwQ/c+tzA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bandapellydhanush/2303A52472/blob/main/ASSIGNMENT_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**ASSIGNMENT-04**\n",
        "\n",
        "#**2303A52472**"
      ],
      "metadata": {
        "id": "IO7swBWGEX4I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. (1 ponto) Design a simple ANN architecture with one input and one output layer (no hidden layer). Assume a linear activation function in the output layer.\n",
        "• Write Python code for a backpropagation algorithm with gradient descent optimization to update weights and bias parameters of the ANN model with training data shown in Table 1.\n",
        "• Calculate the mean square error with training and testing data shown in Table 2.\n",
        "• Write Python code that reads the input data [x1, x2, and x3] from the user. Predict the output with deployed ANN model\n",
        "Tabela 1: Training Data\n",
        "x1 x2 x3\n",
        "y\n",
        "0.1 0.2 0.3 0.14\n",
        "0.2 0.3 0.4 0.20\n",
        "0.3 0.4 0.5 0.26\n",
        "0.5 0.6 0.7 0.38\n",
        "0.1 0.3 0.5 0.22\n",
        "0.2 0.4 0.6 0.28\n",
        "0.3 0.5 0.7 0.34\n",
        "0.4 0.6 0.8 0.40\n",
        "0.5 0.7 0.1 0.22\n",
        "\n",
        "Tabela 2: Test Data\n",
        "\n",
        "x1 x2 x3\n",
        "   y\n",
        "\n",
        "0.6 0.7 0.8 0.44\n",
        "\n",
        "0.7 0.8 0.9 0.50"
      ],
      "metadata": {
        "id": "FfokckoPDpJe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNBGgDPHB1VN",
        "outputId": "304a4402-754f-423d-8b8c-785e9d336f32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 3.1135757598043883\n",
            "Epoch 1000, Loss: 0.036135658002278734\n",
            "Epoch 2000, Loss: 0.017106990507071148\n",
            "Epoch 3000, Loss: 0.008133450106765298\n",
            "Epoch 4000, Loss: 0.0038986461158697036\n",
            "Epoch 5000, Loss: 0.0018982489074906542\n",
            "Epoch 6000, Loss: 0.0009520292952256\n",
            "Epoch 7000, Loss: 0.0005034893947673867\n",
            "Epoch 8000, Loss: 0.0002900820400857512\n",
            "Epoch 9000, Loss: 0.00018786345219003157\n",
            "Training MSE: 0.00013828109769014991\n",
            "Testing MSE: 8.908164963686612e-05\n",
            "Enter x1: 0.5\n",
            "Enter x2: 0.6\n",
            "Enter x3: 0.7\n",
            "Predicted Output: [0.36950089]\n"
          ]
        }
      ],
      "source": [
        "0import numpy as np\n",
        "\n",
        "# Training Data (Tabela 1)\n",
        "X_train = np.array([[0.1, 0.2, 0.3],\n",
        "                   [0.2, 0.3, 0.4],\n",
        "                   [0.3, 0.4, 0.5],\n",
        "                   [0.5, 0.6, 0.7],\n",
        "                   [0.1, 0.3, 0.5],\n",
        "                   [0.2, 0.4, 0.6],\n",
        "                   [0.3, 0.5, 0.7],\n",
        "                   [0.4, 0.6, 0.8],\n",
        "                   [0.5, 0.7, 0.1]])\n",
        "y_train = np.array([0.14, 0.20, 0.26, 0.38, 0.22, 0.28, 0.34, 0.40, 0.22])\n",
        "\n",
        "# Test Data (Tabela 2)\n",
        "X_test = np.array([[0.6, 0.7, 0.8],\n",
        "                  [0.7, 0.8, 0.9]])\n",
        "y_test = np.array([0.44, 0.50])\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.01\n",
        "epochs = 10000\n",
        "\n",
        "# Initialize weights and bias\n",
        "weights = np.random.randn(3)  # 3 weights for 3 features\n",
        "bias = np.random.randn(1)\n",
        "\n",
        "# Linear Activation Function (Output layer)\n",
        "def linear_activation(x):\n",
        "    return x\n",
        "\n",
        "# Mean Squared Error\n",
        "def mse(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred)**2)\n",
        "\n",
        "# Backpropagation Algorithm\n",
        "def backpropagation(X, y, weights, bias, learning_rate, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        # Forward pass\n",
        "        y_pred = linear_activation(np.dot(X, weights) + bias)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = mse(y, y_pred)\n",
        "\n",
        "        # Compute gradients (derivatives)\n",
        "        d_loss = y_pred - y  # derivative of MSE w.r.t predictions\n",
        "        d_weights = np.dot(X.T, d_loss) / len(y)  # derivative of MSE w.r.t weights\n",
        "        d_bias = np.sum(d_loss) / len(y)  # derivative of MSE w.r.t bias\n",
        "\n",
        "        # Update weights and bias using gradient descent\n",
        "        weights -= learning_rate * d_weights\n",
        "        bias -= learning_rate * d_bias\n",
        "\n",
        "        if epoch % 1000 == 0:\n",
        "            print(f'Epoch {epoch}, Loss: {loss}')\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "# Train the model using backpropagation\n",
        "weights, bias = backpropagation(X_train, y_train, weights, bias, learning_rate, epochs)\n",
        "\n",
        "# Calculate MSE on the training and testing data\n",
        "y_train_pred = linear_activation(np.dot(X_train, weights) + bias)\n",
        "train_mse = mse(y_train, y_train_pred)\n",
        "\n",
        "y_test_pred = linear_activation(np.dot(X_test, weights) + bias)\n",
        "test_mse = mse(y_test, y_test_pred)\n",
        "\n",
        "print(f'Training MSE: {train_mse}')\n",
        "print(f'Testing MSE: {test_mse}')\n",
        "\n",
        "# Predict output for user input\n",
        "def predict_input():\n",
        "    x1 = float(input(\"Enter x1: \"))\n",
        "    x2 = float(input(\"Enter x2: \"))\n",
        "    x3 = float(input(\"Enter x3: \"))\n",
        "    prediction = linear_activation(np.dot([x1, x2, x3], weights) + bias)\n",
        "    print(f\"Predicted Output: {prediction}\")\n",
        "\n",
        "# Predict output for a new input\n",
        "predict_input()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. (1 ponto) Design a simple ANN architecture with one input and one output layer (no hidden\n",
        "layer). Assume a sigmoid activation function shown in the equation 1 in the output layer.\n",
        "f(x) = 1\n",
        "1 + e\n",
        "−x\n",
        "(1)\n",
        "• Write Python code for a backpropagation algorithm with gradient descent optimization to\n",
        "update weights and bias parameters of the ANN model with training data shown in Table\n",
        "Tabela 3: Training Data\n",
        "x1 x2 x3 y\n",
        "0.1 0.2 0.3 0.5349\n",
        "0.2 0.3 0.4 0.5498\n",
        "0.3 0.4 0.5 0.5646\n",
        "0.5 0.6 0.7 0.5939\n",
        "0.1 0.3 0.5 0.5548\n",
        "0.2 0.4 0.6 0.5695\n",
        "0.3 0.5 0.7 0.5842\n",
        "0.4 0.6 0.8 0.5987\n",
        "0.5 0.7 0.1 0.5548\n",
        "Tabela 4: Test Data\n",
        "x1 x2 x3 y\n",
        "0.6 0.7 0.8 0.6083\n",
        "0.7 0.8 0.9 0.6225\n",
        "\n",
        "• Calculate the mean square error with training and testing data shown in Table 4.\n",
        "• Write Python code that reads the input data [x1, x2, and x3] from the user. Predict the\n",
        "output with deployed ANN model\n",
        "• Expected learning Outcomes from this assignment related to python\n",
        "– Students are able to understand how backpropagation algorithm helps to update model\n",
        "parameters of ANN\n",
        "– Students are able to write code in python for backpropagation algorithm\n",
        "– Students are able to design architecture of ANN based on problem statement\n",
        "– Students are able to derive mathematical expression for change in weights and bias\n",
        "parameters for different activation functions\n",
        "• Naming cinvention"
      ],
      "metadata": {
        "id": "Mjyd1TA_ExCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Training Data (Tabela 3)\n",
        "X_train = np.array([[0.1, 0.2, 0.3],\n",
        "                   [0.2, 0.3, 0.4],\n",
        "                   [0.3, 0.4, 0.5],\n",
        "                   [0.5, 0.6, 0.7],\n",
        "                   [0.1, 0.3, 0.5],\n",
        "                   [0.2, 0.4, 0.6],\n",
        "                   [0.3, 0.5, 0.7],\n",
        "                   [0.4, 0.6, 0.8],\n",
        "                   [0.5, 0.7, 0.1]])\n",
        "y_train = np.array([0.5349, 0.5498, 0.5646, 0.5939, 0.5548, 0.5695, 0.5842, 0.5987, 0.5548])\n",
        "\n",
        "# Test Data (Tabela 4)\n",
        "X_test = np.array([[0.6, 0.7, 0.8],\n",
        "                  [0.7, 0.8, 0.9]])\n",
        "y_test = np.array([0.6083, 0.6225])\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.01\n",
        "epochs = 10000\n",
        "\n",
        "# Initialize weights and bias\n",
        "weights = np.random.randn(3)  # 3 weights for 3 features\n",
        "bias = np.random.randn(1)\n",
        "\n",
        "# Sigmoid Activation Function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Derivative of Sigmoid Function\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "# Mean Squared Error\n",
        "def mse(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred)**2)\n",
        "\n",
        "# Backpropagation Algorithm\n",
        "def backpropagation(X, y, weights, bias, learning_rate, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        # Forward pass\n",
        "        y_pred = sigmoid(np.dot(X, weights) + bias)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = mse(y, y_pred)\n",
        "\n",
        "        # Compute gradients (derivatives)\n",
        "        d_loss = y_pred - y  # derivative of MSE w.r.t predictions\n",
        "        d_weights = np.dot(X.T, d_loss * sigmoid_derivative(np.dot(X, weights) + bias)) / len(y)  # derivative of MSE w.r.t weights\n",
        "        d_bias = np.sum(d_loss * sigmoid_derivative(np.dot(X, weights) + bias)) / len(y)  # derivative of MSE w.r.t bias\n",
        "\n",
        "        # Update weights and bias using gradient descent\n",
        "        weights -= learning_rate * d_weights\n",
        "        bias -= learning_rate * d_bias\n",
        "\n",
        "        if epoch % 1000 == 0:\n",
        "            print(f'Epoch {epoch}, Loss: {loss}')\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "# Train the model using backpropagation\n",
        "weights, bias = backpropagation(X_train, y_train, weights, bias, learning_rate, epochs)\n",
        "\n",
        "# Calculate MSE on the training and testing data\n",
        "y_train_pred = sigmoid(np.dot(X_train, weights) + bias)\n",
        "train_mse = mse(y_train, y_train_pred)\n",
        "\n",
        "y_test_pred = sigmoid(np.dot(X_test, weights) + bias)\n",
        "test_mse = mse(y_test, y_test_pred)\n",
        "\n",
        "print(f'Training MSE: {train_mse}')\n",
        "print(f'Testing MSE: {test_mse}')\n",
        "\n",
        "# Predict output for user input\n",
        "def predict_input():\n",
        "    x1 = float(input(\"Enter x1: \"))\n",
        "    x2 = float(input(\"Enter x2: \"))\n",
        "    x3 = float(input(\"Enter x3: \"))\n",
        "    prediction = sigmoid(np.dot([x1, x2, x3], weights) + bias)\n",
        "    print(f\"Predicted Output: {prediction}\")\n",
        "\n",
        "# Predict output for a new input\n",
        "predict_input()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcFwzvqeFGHx",
        "outputId": "2727bbe8-3931-485e-b1f8-1e81eec29835"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.016953543930994127\n",
            "Epoch 1000, Loss: 0.004217218300903165\n",
            "Epoch 2000, Loss: 0.001349322960270403\n",
            "Epoch 3000, Loss: 0.000822995082700373\n",
            "Epoch 4000, Loss: 0.0007185772851397787\n",
            "Epoch 5000, Loss: 0.0006842532827716545\n",
            "Epoch 6000, Loss: 0.0006618531286515176\n",
            "Epoch 7000, Loss: 0.0006420603739061331\n",
            "Epoch 8000, Loss: 0.0006233752991526571\n",
            "Epoch 9000, Loss: 0.0006055322608767358\n",
            "Training MSE: 0.0005884604765994863\n",
            "Testing MSE: 0.003744791278091686\n",
            "Enter x1: 0.7\n",
            "Enter x2: 0.8\n",
            "Enter x3: 0.9\n",
            "Predicted Output: [0.55504104]\n"
          ]
        }
      ]
    }
  ]
}