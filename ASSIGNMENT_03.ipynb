{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyNeun9Mooqxds7G6RQeqE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bandapellydhanush/2303A52472/blob/main/ASSIGNMENT_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**ASSIGNMENT-03**\n",
        "#**2303A52472**"
      ],
      "metadata": {
        "id": "EhFoGHtH-gBx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. (1 ponto) Write Python code without using any libraries to find the value of x at which the function f(x) shown in equation (1) has minimum value. Consider Gradient Descent Algorithm.\n",
        "f(x) = 5x² + 3x² + 10"
      ],
      "metadata": {
        "id": "vVSnjOhD-xYa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zK9Cf4or-fSA",
        "outputId": "87aaa16a-20a0-4040-c01a-ee55d6706b4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The minimum value of x is: -4.77519666596786e-07\n"
          ]
        }
      ],
      "source": [
        "def gradient_descent(learning_rate=0.1, precision=1e-6, max_iterations=1000):\n",
        "    x = 10  # Initial guess\n",
        "    for _ in range(max_iterations):\n",
        "        gradient = 16 * x  # Derivative of f(x)\n",
        "        new_x = x - learning_rate * gradient  # Update x\n",
        "\n",
        "        # Stopping condition\n",
        "        if abs(new_x - x) < precision:\n",
        "            break\n",
        "\n",
        "        x = new_x\n",
        "\n",
        "    return x\n",
        "\n",
        "x_min = gradient_descent()\n",
        "print(\"The minimum value of x is:\", x_min)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. (1 ponto) Write Python code without using any libraries to find the value of x and y at which the function g(x, y) shown in equation (2) has minimum value. Consider Gradient Descent Algorithm. f(x) = 3x ^ 2 + 5e ^ (- y) + 10"
      ],
      "metadata": {
        "id": "N7rbG32W_FOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(learning_rate=0.1, precision=1e-6, max_iterations=1000):\n",
        "    x, y = 10, 10  # Initial guesses\n",
        "    for _ in range(max_iterations):\n",
        "        gradient_x = 6 * x  # Partial derivative w.r.t x\n",
        "        gradient_y = -5 * (2.71828 ** -y)  # Partial derivative w.r.t y (approximating e as 2.71828)\n",
        "\n",
        "        new_x = x - learning_rate * gradient_x  # Update x\n",
        "        new_y = y - learning_rate * gradient_y  # Update y\n",
        "\n",
        "        # Stopping condition\n",
        "        if abs(new_x - x) < precision and abs(new_y - y) < precision:\n",
        "            break\n",
        "\n",
        "        x, y = new_x, new_y\n",
        "\n",
        "    return x, y\n",
        "\n",
        "x_min, y_min = gradient_descent()\n",
        "print(\"The minimum value of x and y are:\", x_min, y_min)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJ_oqciS_F-X",
        "outputId": "d577d03f-4498-42e9-809d-428ac0c2f274"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The minimum value of x and y are: 0.0 10.022446553078426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. (1 ponto) Write Python code without using any libraries to find the value of x at which the sigmoid function z(x) shown in equation (3) has minimum value. Consider Gradient Descent Algorithm.\n",
        "z(x) = 1/(1 + e ^ (- x))."
      ],
      "metadata": {
        "id": "0BDmI7Y7_Wtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(learning_rate=0.1, precision=1e-6, max_iterations=1000):\n",
        "    x = 10  # Initial guess\n",
        "    for _ in range(max_iterations):\n",
        "        gradient_x = (2.71828 ** -x) / ((1 + 2.71828 ** -x) ** 2)  # Derivative of sigmoid function\n",
        "\n",
        "        new_x = x - learning_rate * gradient_x  # Update x\n",
        "\n",
        "        # Stopping condition\n",
        "        if abs(new_x - x) < precision:\n",
        "            break\n",
        "\n",
        "        x = new_x\n",
        "\n",
        "    return x\n",
        "\n",
        "x_min = gradient_descent()\n",
        "print(\"The minimum value of x is:\", x_min)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzZCF66u_n6x",
        "outputId": "94d01c26-a3e3-419f-f828-6c76757c18cc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The minimum value of x is: 9.995450064690546\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. (1 ponto) Write Python code without using any libraries to find the value of optimal values of model parameters M and C such that the model's Square Error Value shown in equation 4 will be minimum. It means model gives output close to expected output as show\n",
        "SE= (ExpectedOutput - PredictedOutput)²\n",
        "• Expected Leaning Outcomes from this assignment related to python\n",
        "Students are able to understand gradient descent algorithm to solve both single and\n",
        "multi variable unconstrained non linear optimization problems\n",
        "Students are able to write code in python for gradient descent algorithm\n",
        "\n",
        "  • Naming cinvention"
      ],
      "metadata": {
        "id": "KPKR8qqgAGZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data (x, ExpectedOutput)\n",
        "data = [(1, 5), (2, 7), (3, 9), (4, 11)]\n",
        "\n",
        "# Function definition for SE\n",
        "def SE(M, C, data):\n",
        "    error = 0\n",
        "    for x, expected in data:\n",
        "        predicted = M * x + C\n",
        "        error += (expected - predicted)**2\n",
        "    return error\n",
        "\n",
        "# Derivatives of SE\n",
        "def dSE_dM(M, C, data):\n",
        "    grad_M = 0\n",
        "    for x, expected in data:\n",
        "        predicted = M * x + C\n",
        "        grad_M += -2 * x * (expected - predicted)\n",
        "    return grad_M\n",
        "\n",
        "def dSE_dC(M, C, data):\n",
        "    grad_C = 0\n",
        "    for x, expected in data:\n",
        "        predicted = M * x + C\n",
        "        grad_C += -2 * (expected - predicted)\n",
        "    return grad_C\n",
        "\n",
        "# Gradient Descent Algorithm to minimize SE\n",
        "def gradient_descent_linear(data, dSE_dM, dSE_dC, M_start, C_start, learning_rate, num_iterations):\n",
        "    M, C = M_start, C_start\n",
        "    for _ in range(num_iterations):\n",
        "        M = M - learning_rate * dSE_dM(M, C, data)\n",
        "        C = C - learning_rate * dSE_dC(M, C, data)\n",
        "    return M, C\n",
        "\n",
        "# Initial guesses for M and C\n",
        "M_start = 0.0\n",
        "C_start = 0.0\n",
        "learning_rate = 0.01\n",
        "num_iterations = 1000\n",
        "\n",
        "# Find the optimal M and C\n",
        "M_opt, C_opt = gradient_descent_linear(data, dSE_dM, dSE_dC, M_start, C_start, learning_rate, num_iterations)\n",
        "print(f\"Optimal values: M = {M_opt}, C = {C_opt}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_QME9-CAShy",
        "outputId": "b6e50065-b206-4888-af51-922adfe303e3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal values: M = 2.000002363705538, C = 2.999993059618899\n"
          ]
        }
      ]
    }
  ]
}